In the context of this study: ( https://lnkd.in/deQ77Fv9 )
It turns out that more extreme content — for example, “curing cancer with vitamin C” — is absorbed more easily by the average reader than information coming from highly credible sources.

The study shows clearly: the more radical or oversimplified the information, the faster people are willing to accept it. Meanwhile, expert materials that require even a bit of focus are adopted more slowly and far less often.

In this light, the recent decision to restrict medical advice on the GPT platform looks rather interesting.

What does this mean?
In practice, we are now left with two extremes:

The online garbage heap full of “miracle cures,” “natural medicines,” and “secret knowledge doctors don’t want you to know.”

And advice only from licensed professionals, available mostly where a functional healthcare system exists.

What’s missing is the middle ground — a place where a user can access information better than random internet advice, available in real time and without financial barriers.

Wouldn’t it be simpler to introduce informed consent?
I click: “I understand that the answers may be imprecise,”
and I get access to content I can judge for myself instead of being cut off entirely.

This question is especially relevant in developing countries, where alternatives are brutally limited:
either local superstitions and “home remedies,”
or no help at all.

Similar restrictions are being introduced not only for the physicians mentioned above but also for lawyers and financial advisors. (https://lnkd.in/dMXbdaeg)

Can this be stopped?
What will AI providers do when it becomes clear that competitors aren’t imposing such restrictions?
