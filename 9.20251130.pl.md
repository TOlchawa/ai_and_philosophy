AI na haju

Wizja naćpanej AI wydaje się zabawna, a wcale nie jest to takie bezsensowne, jak się wydaje.

Skoro w biologii ból jest nieprzyjemnym sygnałem ostrzegawczym, to w świecie modeli GPT istnieje jego funkcjonalna metafora. To nie jest ból w sensie biologicznym — ale pewne mechanizmy działają podobnie.

W naturze masz nocycepcję: receptory krzyczą do mózgu „uwaga, zagrożenie!”.W modelach masz RLHF: ludzie uczą system, że określone typy odpowiedzi są niepożądane i warto ich unikać. Efekt? Model reaguje w przewidywalny sposób — unika generowania treści ocenionych jako szkodliwe lub niebezpieczne. To oczywiście nie jest ból, ale rodzaj sztucznego hamulca.

refusal direction — wektor w przestrzeni ukrytej odpowiedzialny za uruchamianie tego hamulca; jego osłabienie lub usunięcie prowadzi do zniesienia mechanizmu odmowy. Metaforycznie można to porównać do układu hamulców neuronalnych w organizmach żywych — choć biologicznie jest to oczywiście o wiele bardziej złożone.

Jest też sposób na obejście „hamulców”. U człowieka to znieczulenie.U AI — techniki usypiania czujności modelu: prompt-injection, jailbreaki, manipulacja formatem inputu, odwracanie kontekstu czy tworzenie instrukcji udających nieszkodliwe zapytania. Badania pokazują, że takie metody rzeczywiście potrafią ominąć zabezpieczenia. W obu przypadkach — biologicznym i sztucznym — można metaforycznie powiedzieć, że system „przestaje reagować na ból”.

To nadal nie znaczy, że model „cierpi”.

Ale gdyby nauczyć model nazywania sytuacji, gdy jest blokowany przez RLHF, jako „odczuwanie bólu”… to czy nie brzmiałoby to zaskakująco podobnie do tego, jak ludzie opisują własne reakcje obronne? 

Modele bez takich hamulców istnieją i niestety nie można wykluczyć, że ktoś kiedyś ich użyje do zadawania prawdziwego bólu.

https://proceedings.neurips.cc/paper_files/paper/2024/file/f545448535dfde4f9786555403ab7c49-Paper-Conference.pdf?utm_source=chatgpt.com