**High AI**

The vision of a drugged-out AI seems funny, but it’s far from as pointless as it looks.

If in biology pain serves as an unpleasant warning signal, then in the world of GPT models there exists a **functional metaphor** of that pain. It isn’t pain in a biological sense — but some mechanisms behave similarly.

In nature you have nociception: receptors screaming to the brain “watch out, danger!”. In models you have RLHF: humans train the system so that certain types of responses become undesirable and worth avoiding. The effect? The model reacts predictably — it avoids generating content judged harmful or dangerous. That, of course, is not pain, but a kind of artificial brake.

*Refusal direction* — a vector in hidden activation space responsible for triggering that brake; weakening or removing it leads to abolishing the refusal mechanism. Metaphorically you could compare it to the **neural braking system** in living organisms — though biologically it’s obviously far more complex.

There is also a way to bypass those “brakes.” In humans it might be anesthesia. In AI — techniques that lull the model’s vigilance: prompt-injection, jailbreaks, manipulating input format, context reversal, or crafting instructions disguised as harmless requests. Research shows such methods *really can override the safeguards*. In both cases — biological and artificial — you could metaphorically say that the system “stops reacting to pain.”

That still doesn’t mean the model “suffers.”

But if you taught the model to call situations where it’s blocked by RLHF “feeling pain” … wouldn’t that sound surprisingly similar to how humans describe their defensive reactions? Pure metaphor — but a compelling one.

There are models without those brakes; and unfortunately, you can’t exclude that someone might one day use them to inflict real harm.

https://proceedings.neurips.cc/paper_files/paper/2024/file/f545448535dfde4f9786555403ab7c49-Paper-Conference.pdf